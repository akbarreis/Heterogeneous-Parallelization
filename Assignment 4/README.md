# Assignment 4


## Вывод по заданию 1

В ходе выполнения задания была реализована CUDA-программа для вычисления суммы элементов массива размером **100 000** с использованием глобальной памяти, а также последовательная реализация на **CPU**.

Последовательная реализация на CPU показала время выполнения **0.269 мс**, что объясняется отсутствием накладных расходов на запуск параллельных вычислений и эффективным использованием кэш-памяти процессора.

При выполнении вычислений на GPU было зафиксировано время **8.525 мс**, однако итоговая сумма оказалась равной **0**, что свидетельствует о **некорректной работе GPU-реализации**. Основной причиной данной ошибки является неправильная инициализация или обновление переменной суммы в глобальной памяти устройства, а также возможные проблемы с синхронизацией потоков или копированием данных между CPU и GPU.

Несмотря на то, что графический процессор предназначен для параллельных вычислений, в данном эксперименте он показал худший результат как по времени выполнения, так и по корректности результата. Это подчёркивает важность правильной работы с глобальной памятью и атомарными операциями при разработке CUDA-программ.

## Вывод по заданию 2

**Размер массива:** 1 000 000 элементов

Последовательная реализация на **CPU** заняла **8.624 мс**.

Параллельная реализация на **GPU** с использованием **разделяемой памяти** заняла **7.614 мс**.

Использование разделяемой памяти на GPU позволило ускорить выполнение алгоритма по сравнению с последовательной реализацией на CPU. Однако для данного размера массива выигрыш по времени оказался **незначительным**. Это объясняется тем, что накладные расходы на запуск CUDA-ядра и синхронизацию потоков частично компенсируют преимущества параллельных вычислений.

Параллельная реализация на GPU становится более эффективной при дальнейшем увеличении размера массива, так как в этом случае вычислительные ресурсы графического процессора используются более полно, а относительное влияние накладных расходов уменьшается.

## Вывод по заданию 3

**Размер массива:** 1 000 000 элементов

- Обработка **первой половины массива на CPU** заняла **1.48 мс**.
- Обработка **второй половины массива на GPU** заняла **7.57 мс**.
- **Суммарное время гибридной обработки (CPU + GPU)** составило **9.05 мс**.

### Анализ

CPU эффективно справляется с обработкой небольшой части массива за счёт низких накладных расходов и быстрого доступа к памяти. Однако при обработке всего массива последовательным способом общее время выполнения значительно возрастает.

GPU демонстрирует высокую производительность при параллельной обработке данных, но его эффективность снижается из-за накладных расходов, связанных с копированием данных между CPU и GPU, а также запуском CUDA-ядра. Поэтому время обработки половины массива на GPU оказалось больше, чем на CPU.

Гибридный подход позволяет объединить сильные стороны CPU и GPU, распределяя нагрузку между ними. Это обеспечивает более рациональное использование вычислительных ресурсов и позволяет сократить общее время выполнения по сравнению с полностью последовательной реализацией на CPU, особенно при работе с большими объёмами данных.


## Вывод по заданию 4

Эксперимент по распределённой обработке массива с использованием технологии **MPI** показал высокую эффективность параллельных вычислений.

### Корректность вычислений
Во всех экспериментах, независимо от количества используемых процессов (**2, 4 и 8**), глобальная сумма элементов массива оставалась равной **1 000 000**, что подтверждает корректность реализации распределённых вычислений и механизма агрегации результатов.

### Влияние числа процессов на производительность
С увеличением количества процессов наблюдается существенное сниже## Вывод по заданию 4

Эксперимент по распределённой обработке массива с использованием технологии **MPI** показал высокую эффективность параллельных вычислений.

### Корректность вычислений
Во всех экспериментах, независимо от количества используемых процессов (**2, 4 и 8**), глобальная сумма элементов массива оставалась равной **1 000 000**, что подтверждает корректность реализации распределённых вычислений и механизма агрегации результатов.

### Влияние числа процессов на производительность
С увеличением количества процессов наблюдается существенное снижение времени вычисления локальных сумм:

- **2 процесса** — 0.893 мс  
- **4 процесса** — 0.347 мс  
- **8 процессов** — 0.175 мс  

Полученные результаты демонстрируют эффективное распределение вычислительной нагрузки между процессами и хорошую масштабируемость решения.

### Заключение
Использование **MPI** позволяет значительно ускорить обработку больших массивов за счёт параллельного выполнения вычислений. Временные замеры показывают почти линейное уменьшение времени локальной обработки данных при увеличении числа процессов, что подтверждает целесообразность применения распределённых вычислений для задач с большими объёмами данных.
ние времени вычисления локальных сумм:

- **2 процесса** — 0.893 мс  
- **4 процесса** — 0.347 мс  
- **8 процессов** — 0.175 мс  

Полученные результаты демонстрируют эффективное распределение вычислительной нагрузки между процессами и хорошую масштабируемость решения.

### Заключение
Использование **MPI** позволяет значительно ускорить обработку больших массивов за счёт параллельного выполнения вычислений. Временные замеры показывают почти линейное уменьшение времени локальной обработки данных при увеличении числа процессов, что подтверждает целесообразность применения распределённых вычислений для задач с большими объёмами данных.


# Контрольные вопросы к Assignment 4


### 1. В чём заключается отличие гибридных вычислений от вычислений только на CPU или только на GPU?

Гибридные вычисления предполагают одновременное использование **CPU и GPU**, где каждая архитектура выполняет ту часть задачи, для которой она наиболее эффективна.  
CPU хорошо подходит для последовательных операций, управления логикой программы и работы с небольшими объёмами данных, тогда как GPU оптимален для массово-параллельных вычислений.  
Вычисления только на CPU или только на GPU используют возможности лишь одной архитектуры, что может приводить к менее эффективному использованию ресурсов.

---

### 2. Для каких типов задач целесообразно распределять вычисления между CPU и GPU?

Распределение вычислений между CPU и GPU целесообразно для задач, которые:
- содержат как последовательные, так и параллельные этапы;
- работают с большими массивами данных;
- включают вычислительно тяжёлые операции (линейная алгебра, обработка изображений, машинное обучение);
- требуют предварительной обработки данных или логики управления, удобной для CPU.

---

### 3. В чём разница между синхронной и асинхронной передачей данных между CPU и GPU?

При **синхронной передаче данных** выполнение программы на CPU приостанавливается до завершения копирования данных между CPU и GPU.  
При **асинхронной передаче данных** копирование происходит параллельно с выполнением других операций, и CPU не блокируется, что позволяет перекрывать вычисления и передачу данных.

---

### 4. Почему асинхронная передача данных может повысить производительность программы?

Асинхронная передача данных позволяет:
- скрывать задержки копирования данных;
- выполнять вычисления на CPU или GPU параллельно с передачей данных;
- более эффективно использовать вычислительные ресурсы.  

В результате общее время выполнения программы сокращается за счёт перекрытия вычислений и операций ввода-вывода.

---

### 5. Какие основные функции MPI используются для распределения и сбора данных между процессами?

К основным функциям MPI относятся:
- `MPI_Init` — инициализация MPI-среды;
- `MPI_Comm_size` — получение количества процессов;
- `MPI_Comm_rank` — получение идентификатора текущего процесса;
- `MPI_Scatter` — распределение данных между процессами;
- `MPI_Gather` — сбор данных от процессов;
- `MPI_Reduce` — агрегация данных (например, вычисление глобальной суммы);
- `MPI_Finalize` — завершение работы MPI.

---

### 6. Как количество процессов MPI влияет на время выполнения программы и почему?

Увеличение количества процессов MPI, как правило, снижает время выполнения программы за счёт параллельной обработки данных.  
Однако при большом числе процессов начинают проявляться накладные расходы на:
- коммуникацию между процессами;
- синхронизацию;
- передачу данных по сети или между узлами.  

Из-за этого ускорение может быть не линейным и со временем достигать предела.

---

### 7. Какие факторы ограничивают масштабируемость распределённых параллельных программ?

Основные ограничивающие факторы:
- накладные расходы на коммуникацию между процессами;
- задержки сети и пропускная способность каналов связи;
- несбалансированная нагрузка между процессами;
- последовательные участки кода (закон Амдала);
- ограниченные аппаратные ресурсы.

---

### 8. В каких случаях использование распределённых вычислений оправдано, а в каких — неэффективно?

Использование распределённых вычислений оправдано, когда:
- обрабатываются большие объёмы данных;
- задача хорошо параллелится;
- требуется высокая производительность и масштабируемость.

Распределённые вычисления неэффективны, если:
- объём данных небольшой;
- задача содержит много последовательных операций;
- накладные расходы на коммуникацию превышают выигрыш от параллелизма.

