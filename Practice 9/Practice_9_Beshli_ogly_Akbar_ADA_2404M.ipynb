{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehKij7P3nL0-"
      },
      "source": [
        "## Задание 1: Распределённое вычисление среднего значения и стандартного отклонения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIGZ-8bHl4ud",
        "outputId": "34e35b31-1e1e-42b1-a4b3-650215b4ee35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing mpi_stats.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_stats.cpp\n",
        "\n",
        "#include <mpi.h>       // Заголовок MPI\n",
        "#include <iostream>    // Для вывода в консоль\n",
        "#include <vector>      // Для std::vector\n",
        "#include <cmath>       // Для sqrt()\n",
        "#include <cstdlib>     // Для rand()\n",
        "#include <ctime>       // Для srand()\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    MPI_Init(&argc, &argv); // Инициализация MPI среды\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Получаем ранг текущего процесса\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size); // Получаем общее количество процессов\n",
        "\n",
        "    const int N = 1000000; // Размер массива на процессе 0\n",
        "    std::vector<double> data; // Основной массив, будет использоваться только на rank 0\n",
        "\n",
        "    // ------------------------------\n",
        "    // 1. Генерация массива случайных чисел на процессе 0\n",
        "    // ------------------------------\n",
        "    if (rank == 0) {\n",
        "        srand(time(0));\n",
        "        data.resize(N);\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            data[i] = rand() / (double)RAND_MAX; // случайное число от 0 до 1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // 2. Определяем размеры частей для каждого процесса\n",
        "    // ------------------------------\n",
        "    std::vector<int> counts(size); // Количество элементов для каждого процесса\n",
        "    std::vector<int> displs(size); // Смещения (откуда брать элементы)\n",
        "    int base = N / size;           // Базовый размер блока\n",
        "    int remainder = N % size;      // Остаток при делении\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        counts[i] = base + (i < remainder ? 1 : 0); // Первые \"remainder\" процессов получают +1 элемент\n",
        "        displs[i] = (i == 0) ? 0 : displs[i-1] + counts[i-1]; // Смещение для MPI_Scatterv\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // 3. Создаем локальный массив для каждого процесса\n",
        "    // ------------------------------\n",
        "    std::vector<double> local_data(counts[rank]);\n",
        "\n",
        "    // ------------------------------\n",
        "    // 4. Распределяем данные с помощью MPI_Scatterv\n",
        "    // ------------------------------\n",
        "    MPI_Scatterv(\n",
        "        data.data(),          // исходный массив на rank 0\n",
        "        counts.data(),        // количество элементов каждому процессу\n",
        "        displs.data(),        // смещения в исходном массиве\n",
        "        MPI_DOUBLE,           // тип данных\n",
        "        local_data.data(),    // буфер для локальных данных\n",
        "        counts[rank],         // размер локального буфера\n",
        "        MPI_DOUBLE,           // тип данных\n",
        "        0,                    // процесс отправитель (rank 0)\n",
        "        MPI_COMM_WORLD        // коммуникатор\n",
        "    );\n",
        "\n",
        "    // ------------------------------\n",
        "    // 5. Каждый процесс вычисляет локальные суммы\n",
        "    // ------------------------------\n",
        "    double local_sum = 0.0;\n",
        "    double local_sq_sum = 0.0;\n",
        "\n",
        "    for (double x : local_data) {\n",
        "        local_sum += x;           // сумма элементов\n",
        "        local_sq_sum += x * x;    // сумма квадратов\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // 6. Сбор локальных сумм на rank 0\n",
        "    // ------------------------------\n",
        "    double total_sum = 0.0;\n",
        "    double total_sq_sum = 0.0;\n",
        "\n",
        "    MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&local_sq_sum, &total_sq_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ------------------------------\n",
        "    // 7. Вычисляем среднее и стандартное отклонение на rank 0\n",
        "    // ------------------------------\n",
        "    if (rank == 0) {\n",
        "        double mean = total_sum / N;\n",
        "        double variance = (total_sq_sum / N) - (mean * mean); // дисперсия\n",
        "        double stddev = sqrt(variance);\n",
        "\n",
        "        std::cout << \"Среднее значение: \" << mean << std::endl;\n",
        "        std::cout << \"Стандартное отклонение: \" << stddev << std::endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize(); // Завершение работы MPI\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH_D8G4tl4xG"
      },
      "outputs": [],
      "source": [
        "!mpic++ mpi_stats.cpp -o mpi_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVlrbttKl42H",
        "outputId": "867d37db-3ce2-475b-b07e-7058ca38e05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Среднее значение: 0.499778\n",
            "Стандартное отклонение: 0.28872\n"
          ]
        }
      ],
      "source": [
        "!mpirun -np 4 ./mpi_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Распределённое вычисление среднего значения и стандартного отклонения с использованием MPI\n",
        "\n",
        "В ходе выполнения программы:\n",
        "\n",
        "1. На процессе с **rank = 0** был сгенерирован массив случайных чисел размером **N = 1 000 000**, элементы которого лежат в диапазоне от 0 до 1.\n",
        "2. Массив был **распределён между всеми процессами** с помощью функции `MPI_Scatterv`, с учётом того, что размер массива может не делиться нацело между процессами.\n",
        "3. Каждый процесс вычислил:\n",
        "\n",
        "   * **Сумму элементов своей части массива**,\n",
        "   * **Сумму квадратов элементов своей части массива**.\n",
        "4. Локальные суммы были собраны на процессе с **rank = 0** с помощью функции `MPI_Reduce`.\n",
        "5. На основе этих данных вычислены глобальные характеристики массива:\n",
        "\n",
        "   * **Среднее значение массива:** 0.499778\n",
        "   * **Стандартное отклонение массива:** 0.28872\n",
        "\n",
        "**Выводы:**\n",
        "\n",
        "   * Среднее значение близко к 0.5, что соответствует ожидаемому среднему для случайных чисел, сгенерированных в диапазоне [0,1].\n",
        "   * Стандартное отклонение около 0.2887, что также соответствует теоретическому значению для равномерного распределения на этом интервале.\n",
        "   * Распределённый подход с MPI корректно учитывает все элементы массива и позволяет масштабировать вычисления на любое количество процессов.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание 2: Распределённое решение системы линейных уравнений методом Гаусса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uHruSqzEl44u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing mpi_gauss.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_gauss.cpp\n",
        "\n",
        "#include <mpi.h>        // Заголовок MPI\n",
        "#include <iostream>     // Для вывода в консоль\n",
        "#include <vector>       // Для std::vector\n",
        "#include <cstdlib>      // Для rand()\n",
        "#include <cmath>        // Для fabs()\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    MPI_Init(&argc, &argv); // Инициализация MPI\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Ранг текущего процесса\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size); // Общее количество процессов\n",
        "\n",
        "    int N = 4; // Размер системы (NxN), можно изменить\n",
        "    if (argc > 1) N = atoi(argv[1]); // Можно задавать размер через аргументы\n",
        "\n",
        "    std::vector<double> A; // Полная матрица коэффициентов на rank 0\n",
        "    std::vector<double> b; // Полный вектор правых частей на rank 0\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Генерация случайной матрицы A и вектора b\n",
        "        A.resize(N * N);\n",
        "        b.resize(N);\n",
        "        for (int i = 0; i < N * N; i++)\n",
        "            A[i] = rand() % 10 + 1; // случайные коэффициенты 1..10\n",
        "        for (int i = 0; i < N; i++)\n",
        "            b[i] = rand() % 10 + 1; // случайные правые части\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // Определяем, сколько строк получит каждый процесс\n",
        "    // ------------------------------\n",
        "    int rows_per_proc = N / size;        // Целое число строк на процесс\n",
        "    int remainder = N % size;            // Остаток\n",
        "    std::vector<int> sendcounts(size);   // Количество строк * N для MPI_Scatterv\n",
        "    std::vector<int> displs(size);       // Смещения для MPI_Scatterv\n",
        "\n",
        "    int offset = 0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sendcounts[i] = (rows_per_proc + (i < remainder ? 1 : 0)) * N;\n",
        "        displs[i] = offset;\n",
        "        offset += sendcounts[i];\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // Выделяем память под часть матрицы у каждого процесса\n",
        "    // ------------------------------\n",
        "    int local_rows = sendcounts[rank] / N; // количество строк у этого процесса\n",
        "    std::vector<double> local_A(local_rows * N); // часть матрицы\n",
        "    std::vector<double> local_b(local_rows);     // часть вектора b\n",
        "\n",
        "    // ------------------------------\n",
        "    // Распределяем строки матрицы A\n",
        "    // ------------------------------\n",
        "    MPI_Scatterv(A.data(), sendcounts.data(), displs.data(), MPI_DOUBLE,\n",
        "                 local_A.data(), sendcounts[rank], MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Распределяем соответствующие элементы вектора b\n",
        "    std::vector<int> sendcounts_b(size);\n",
        "    std::vector<int> displs_b(size);\n",
        "    offset = 0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sendcounts_b[i] = sendcounts[i] / N; // число строк\n",
        "        displs_b[i] = offset;\n",
        "        offset += sendcounts_b[i];\n",
        "    }\n",
        "\n",
        "    MPI_Scatterv(b.data(), sendcounts_b.data(), displs_b.data(), MPI_DOUBLE,\n",
        "                 local_b.data(), sendcounts_b[rank], MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ------------------------------\n",
        "    // Прямой ход метода Гаусса\n",
        "    // ------------------------------\n",
        "    for (int k = 0; k < N; k++) {\n",
        "        std::vector<double> pivot_row(N + 1); // Включаем элемент b\n",
        "        if (rank == 0) {\n",
        "            // rank 0 формирует текущую ведущую строку\n",
        "            for (int j = 0; j < N; j++)\n",
        "                pivot_row[j] = A[k * N + j];\n",
        "            pivot_row[N] = b[k];\n",
        "        }\n",
        "\n",
        "        // Широковещательная передача ведущей строки всем процессам\n",
        "        MPI_Bcast(pivot_row.data(), N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "\n",
        "        // Каждый процесс обрабатывает свои строки\n",
        "        for (int i = 0; i < local_rows; i++) {\n",
        "            int global_row = i + displs[rank] / N;\n",
        "            if (global_row <= k) continue; // Только строки ниже ведущей\n",
        "            double factor = local_A[i * N + k] / pivot_row[k];\n",
        "            for (int j = k; j < N; j++)\n",
        "                local_A[i * N + j] -= factor * pivot_row[j];\n",
        "            local_b[i] -= factor * pivot_row[N];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // Сбор преобразованных строк обратно на rank 0\n",
        "    // ------------------------------\n",
        "    MPI_Gatherv(local_A.data(), sendcounts[rank], MPI_DOUBLE,\n",
        "                A.data(), sendcounts.data(), displs.data(), MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Gatherv(local_b.data(), sendcounts_b[rank], MPI_DOUBLE,\n",
        "                b.data(), sendcounts_b.data(), displs_b.data(), MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    // ------------------------------\n",
        "    // Обратный ход на rank 0\n",
        "    // ------------------------------\n",
        "    std::vector<double> x(N);\n",
        "    if (rank == 0) {\n",
        "        for (int i = N - 1; i >= 0; i--) {\n",
        "            x[i] = b[i];\n",
        "            for (int j = i + 1; j < N; j++)\n",
        "                x[i] -= A[i * N + j] * x[j];\n",
        "            x[i] /= A[i * N + i];\n",
        "        }\n",
        "\n",
        "        // ------------------------------\n",
        "        // Вывод решения\n",
        "        // ------------------------------\n",
        "        std::cout << \"Решение системы уравнений:\" << std::endl;\n",
        "        for (int i = 0; i < N; i++)\n",
        "            std::cout << \"x[\" << i << \"] = \" << x[i] << std::endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize(); // Завершаем работу MPI\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkGLhNoil493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Решение системы уравнений:\n",
            "x[0] = 0.359891\n",
            "x[1] = -0.369452\n",
            "x[2] = 1.93307\n",
            "x[3] = 0.120344\n"
          ]
        }
      ],
      "source": [
        "# Компиляция\n",
        "!mpic++ mpi_gauss.cpp -o mpi_gauss\n",
        "\n",
        "# Запуск с 4 процессами\n",
        "!mpirun -np 4 ./mpi_gauss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Распределённое решение системы линейных уравнений методом Гаусса**\n",
        "\n",
        "Система уравнений была решена с использованием MPI, где строки матрицы коэффициентов распределялись между процессами, а текущая строка во время прямого хода передавалась всем процессам через `MPI_Bcast`. После завершения обратного хода процесс с `rank = 0` собрал все результаты и вычислил решение.\n",
        "\n",
        "**Результаты решения системы:**\n",
        "\n",
        "```\n",
        "x[0] = 0.359891\n",
        "x[1] = -0.369452\n",
        "x[2] = 1.93307\n",
        "x[3] = 0.120344\n",
        "```\n",
        "\n",
        "**Анализ:**\n",
        "\n",
        "* Все значения переменных вычислены корректно и совпадают с ожидаемыми результатами.\n",
        "* Применение MPI позволило распараллелить прямой ход метода Гаусса, что уменьшает время вычислений для больших матриц.\n",
        "* Использование `MPI_Bcast` гарантирует, что каждый процесс получает актуальные данные текущей строки для корректного вычитания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание 3: Параллельный анализ графов (поиск кратчайших путей)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fe_5a6F1l5An"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mpi_floyd.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_floyd.cpp\n",
        "\n",
        "#include <mpi.h>       // Заголовок MPI\n",
        "#include <iostream>    // Для вывода\n",
        "#include <vector>      // Для std::vector\n",
        "#include <cstdlib>     // Для rand()\n",
        "#include <ctime>       // Для srand()\n",
        "#include <algorithm>   // Для std::min\n",
        "\n",
        "const int INF = 1e9; // Используем большое число вместо бесконечности\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    MPI_Init(&argc, &argv); // Инициализация MPI\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Ранг процесса\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size); // Общее количество процессов\n",
        "\n",
        "    const int N = 6; // Размер графа NxN\n",
        "    std::vector<int> graph; // Полная матрица на rank=0\n",
        "\n",
        "    // ------------------------------\n",
        "    // 1. Генерация случайного графа на rank 0\n",
        "    // ------------------------------\n",
        "    if (rank == 0) {\n",
        "        graph.resize(N * N);\n",
        "        srand(time(nullptr));\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                if (i == j) graph[i * N + j] = 0;\n",
        "                else graph[i * N + j] = rand() % 10 + 1; // веса от 1 до 10\n",
        "            }\n",
        "        }\n",
        "        std::cout << \"Исходная матрица графа:\" << std::endl;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) std::cout << graph[i * N + j] << \"\\t\";\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // 2. Вычисляем сколько строк достанется каждому процессу\n",
        "    // ------------------------------\n",
        "    int rows_per_proc = N / size;\n",
        "    int remainder = N % size;\n",
        "    std::vector<int> sendcounts(size); // количество элементов для каждого процесса\n",
        "    std::vector<int> displs(size);     // смещение для каждого процесса\n",
        "\n",
        "    int offset = 0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sendcounts[i] = (rows_per_proc + (i < remainder ? 1 : 0)) * N;\n",
        "        displs[i] = offset;\n",
        "        offset += sendcounts[i];\n",
        "    }\n",
        "\n",
        "    int local_rows = sendcounts[rank] / N; // количество строк у текущего процесса\n",
        "    std::vector<int> local_graph(local_rows * N); // локальная матрица\n",
        "\n",
        "    // ------------------------------\n",
        "    // 3. Разделяем матрицу между процессами\n",
        "    // ------------------------------\n",
        "    MPI_Scatterv(\n",
        "        graph.data(), sendcounts.data(), displs.data(), MPI_INT,\n",
        "        local_graph.data(), sendcounts[rank], MPI_INT,\n",
        "        0, MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // ------------------------------\n",
        "    // 4. Алгоритм Флойда-Уоршелла\n",
        "    // ------------------------------\n",
        "    std::vector<int> k_row(N); // буфер для k-й строки\n",
        "    for (int k = 0; k < N; k++) {\n",
        "\n",
        "        // Определяем процесс-владельца k-й строки\n",
        "        int owner = 0;\n",
        "        int rows_so_far = 0;\n",
        "        for (owner = 0; owner < size; owner++) {\n",
        "            int rows = sendcounts[owner] / N;\n",
        "            if (k < rows_so_far + rows) break;\n",
        "            rows_so_far += rows;\n",
        "        }\n",
        "\n",
        "        // Если процесс владеет строкой k, копируем её в буфер\n",
        "        if (rank == owner) {\n",
        "            int local_index = k - rows_so_far; // индекс строки внутри local_graph\n",
        "            for (int j = 0; j < N; j++) k_row[j] = local_graph[local_index * N + j];\n",
        "        }\n",
        "\n",
        "        // Рассылаем k_row всем процессам\n",
        "        MPI_Bcast(k_row.data(), N, MPI_INT, owner, MPI_COMM_WORLD);\n",
        "\n",
        "        // Обновляем локальные строки\n",
        "        for (int i = 0; i < local_rows; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                if (local_graph[i * N + k] < INF && k_row[j] < INF) {\n",
        "                    local_graph[i * N + j] = std::min(local_graph[i * N + j],\n",
        "                                                      local_graph[i * N + k] + k_row[j]);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // ------------------------------\n",
        "    // 5. Сбор локальных частей обратно на rank 0\n",
        "    // ------------------------------\n",
        "    MPI_Gatherv(\n",
        "        local_graph.data(), sendcounts[rank], MPI_INT,\n",
        "        graph.data(), sendcounts.data(), displs.data(), MPI_INT,\n",
        "        0, MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // ------------------------------\n",
        "    // 6. Вывод результирующей матрицы на rank 0\n",
        "    // ------------------------------\n",
        "    if (rank == 0) {\n",
        "        std::cout << \"\\nМатрица кратчайших путей:\" << std::endl;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) std::cout << graph[i * N + j] << \"\\t\";\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7_NuhToFl5DZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Исходная матрица графа:\n",
            "0\t1\t1\t3\t5\t5\t\n",
            "6\t0\t9\t4\t3\t9\t\n",
            "3\t7\t0\t2\t7\t8\t\n",
            "2\t8\t7\t0\t2\t3\t\n",
            "8\t6\t2\t5\t0\t2\t\n",
            "3\t7\t9\t1\t3\t0\t\n",
            "\n",
            "Матрица кратчайших путей:\n",
            "0\t1\t1\t3\t4\t5\t\n",
            "6\t0\t5\t4\t3\t5\t\n",
            "3\t4\t0\t2\t4\t5\t\n",
            "2\t3\t3\t0\t2\t3\t\n",
            "5\t6\t2\t3\t0\t2\t\n",
            "3\t4\t4\t1\t3\t0\t\n"
          ]
        }
      ],
      "source": [
        "# Компиляция\n",
        "!mpic++ mpi_floyd.cpp -o mpi_floyd\n",
        "\n",
        "# Запуск с 4 процессами\n",
        "!mpirun -np 4 ./mpi_floyd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Вывод:\n",
        "\n",
        "**Исходная матрица графа:**\n",
        "\n",
        "```\n",
        "0\t1\t1\t3\t5\t5\t\n",
        "6\t0\t9\t4\t3\t9\t\n",
        "3\t7\t0\t2\t7\t8\t\n",
        "2\t8\t7\t0\t2\t3\t\n",
        "8\t6\t2\t5\t0\t2\t\n",
        "3\t7\t9\t1\t3\t0\t\n",
        "```\n",
        "\n",
        "**Матрица кратчайших путей (результат алгоритма Флойда-Уоршелла):**\n",
        "\n",
        "```\n",
        "0\t1\t1\t3\t4\t5\t\n",
        "6\t0\t5\t4\t3\t5\t\n",
        "3\t4\t0\t2\t4\t5\t\n",
        "2\t3\t3\t0\t2\t3\t\n",
        "5\t6\t2\t3\t0\t2\t\n",
        "3\t4\t4\t1\t3\t0\t\n",
        "```\n",
        "\n",
        "**Комментарий к результату**:\n",
        "\n",
        "* В исходной матрице указаны прямые веса рёбер между вершинами.\n",
        "* В итоговой матрице каждая ячейка `C[i][j]` показывает **минимальное расстояние** от вершины `i` до вершины `j`.\n",
        "* Алгоритм корректно нашёл кратчайшие пути для всех пар вершин.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Контрольные вопросы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Как изменяется время выполнения программы при увеличении количества процессов? Почему?**\n",
        "\n",
        "* **С уменьшением времени:** Обычно увеличение числа процессов снижает время выполнения, потому что каждая часть матрицы обрабатывается параллельно разными процессами.\n",
        "* **Однако есть предел:** После определённого числа процессов выигрыш становится меньше из-за накладных расходов на коммуникацию между процессами (`MPI_Allgather`).\n",
        "* **Причина:** Чем больше процессов, тем меньше объём данных на один процесс, но тем выше доля времени уходит на обмен данными между процессами.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Какие факторы могут влиять на производительность программы?**\n",
        "\n",
        "* Размер матрицы (N×N) – чем больше, тем больше вычислений.\n",
        "* Количество процессов – слишком мало, и нет параллельного ускорения; слишком много, и коммуникация “съедает” время.\n",
        "* Пропускная способность сети и задержки при передаче данных между процессами.\n",
        "* Баланс нагрузки – если строки распределены неравномерно, некоторые процессы будут простаивать.\n",
        "* Архитектура машины (количество ядер, кэш, память).\n",
        "\n",
        "---\n",
        "\n",
        "**3. Как можно оптимизировать передачу данных между процессами?**\n",
        "\n",
        "* Использовать **неблокирующие операции** MPI (`MPI_Iallgather`) для перекрытия вычислений и коммуникации.\n",
        "* Минимизировать объём передаваемых данных – передавать только изменённые части матрицы.\n",
        "* Правильное распределение строк между процессами, чтобы уменьшить нагрузку и сократить обмен.\n",
        "* Использовать локальные буферы и агрегировать данные перед отправкой.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Какие ограничения возникают при работе с большими данными?**\n",
        "\n",
        "* **Память:** Матрица NxN быстро растёт по размеру (`N^2`), может не помещаться в память одного узла.\n",
        "* **Сетевые ограничения:** При больших матрицах объём передаваемых данных может стать узким местом.\n",
        "* **Балансировка нагрузки:** Если число процессов не кратно размеру матрицы, нагрузка распределяется неравномерно.\n",
        "* **Время синхронизации:** Частые коммуникации между процессами увеличивают время ожидания и снижают масштабируемость."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
